{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb18c6ea-2eed-4e3c-b9c5-6498747e5a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to 'reddit_western_inclusivity.csv' and 'reddit_western_inclusivity.json'.\n"
     ]
    }
   ],
   "source": [
    "import praw\n",
    "import json\n",
    "import csv  \n",
    "from datetime import datetime, timezone\n",
    "\n",
    "# Initialize PRAW Reddit application credentials\n",
    "client = praw.Reddit(\n",
    "    client_id='ZwTyGre7TbxHMGIGVVQYVQ',\n",
    "    client_secret='NbEf85G8ckGdtvShah0-gd0iq8F9uw',\n",
    "    password='Ym9Sf?2gsUK!AsW',\n",
    "    username='Sufficient-Guitar587',\n",
    "    user_agent='client for SNAM2024'\n",
    ")\n",
    "\n",
    "def search_posts(queries, limit=None):\n",
    "    \"\"\"\n",
    "    Searches for posts across all of Reddit that match a list of queries.\n",
    "\n",
    "    Args:\n",
    "        queries (list): The search queries to filter posts.\n",
    "        limit (int): The maximum number of posts to fetch.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of dictionaries where each dictionary contains post data and comments.\n",
    "    \"\"\"\n",
    "    # Initialize list to hold posts\n",
    "    posts = []\n",
    "\n",
    "    try:\n",
    "        # Iterate over each query\n",
    "        for query in queries:\n",
    "            for submission in client.subreddit('all').search(query, sort='relevance', limit=limit):\n",
    "                posts.append(fetch_post_data(submission))\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "    return posts\n",
    "\n",
    "def fetch_post_data(submission):\n",
    "    \"\"\"\n",
    "    Fetches data for a single submission and its comments.\n",
    "\n",
    "    Args:\n",
    "        submission (praw.models.Submission): The submission object to fetch data from.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the post data and comments.\n",
    "    \"\"\"\n",
    "    post_data = {\n",
    "        'title': submission.title,\n",
    "        'score': submission.score,\n",
    "        'created_date': datetime.fromtimestamp(submission.created_utc, tz=timezone.utc).strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        'url': submission.url,\n",
    "        'subreddit': submission.subreddit.display_name,\n",
    "        'comments': []\n",
    "    }\n",
    "\n",
    "    # Fetch all comments from the post\n",
    "    submission.comments.replace_more(limit=0)\n",
    "    for comment in submission.comments.list():\n",
    "        post_data['comments'].append({\n",
    "            'body': comment.body,\n",
    "            'author': comment.author.name if comment.author else '[deleted]',\n",
    "            'created_date': datetime.fromtimestamp(comment.created_utc, tz=timezone.utc).strftime('%Y-%m-%d %H:%M:%S')\n",
    "        })\n",
    "\n",
    "    return post_data\n",
    "\n",
    "def save_to_csv(posts, filename='reddit_data.csv'):\n",
    "    \"\"\"\n",
    "    Saves posts and their comments to a CSV file.\n",
    "\n",
    "    Args:\n",
    "        posts (list): List of posts with comments to save.\n",
    "        filename (str): The name of the CSV file to save the data.\n",
    "    \"\"\"\n",
    "    with open(filename, 'w', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.writer(f)\n",
    "        # Write header in CSV\n",
    "        writer.writerow(['post_title', 'post_score', 'post_created_date', 'post_url', 'post_subreddit', 'comment_body', 'comment_author', 'comment_created_date'])\n",
    "\n",
    "        # Write data\n",
    "        for post in posts:\n",
    "            for comment in post['comments']:\n",
    "                writer.writerow([\n",
    "                    post['title'],\n",
    "                    post['score'],\n",
    "                    post['created_date'],\n",
    "                    post['url'],\n",
    "                    post['subreddit'],\n",
    "                    comment['body'],\n",
    "                    comment['author'],\n",
    "                    comment['created_date']\n",
    "                ])\n",
    "\n",
    "def save_to_json(posts, filename='reddit_data.json'):\n",
    "    \"\"\"\n",
    "    Saves posts and their comments to a JSON file.\n",
    "\n",
    "    Args:\n",
    "        posts (list): List of posts with comments to save.\n",
    "        filename (str): The name of the JSON file to save the data.\n",
    "    \"\"\"\n",
    "    with open(filename, 'w', encoding='utf-8') as json_file:\n",
    "        json.dump(posts, json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "\n",
    "queries = [\n",
    "    \"Fenty Beauty inclusivity\",\n",
    "    \"Urban Decay inclusivity\",\n",
    "    \"Tarte Cosmetics inclusivity\",\n",
    "   \n",
    "]\n",
    "\n",
    "posts = search_posts(queries, limit=100)\n",
    "\n",
    "# Save the fetched posts to a CSV file\n",
    "save_to_csv(posts, filename='reddit_western.csv')\n",
    "\n",
    "# Save the fetched posts to a JSON file\n",
    "save_to_json(posts, filename='reddit_western.json')\n",
    "\n",
    "print(\"Data saved to 'reddit_western_inclusivity.csv' and 'reddit_western_inclusivity.json'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "58c44fde-cf29-42ea-b4e9-c8f4790eb8ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to 'reddit_western_inclusivity.csv' and 'reddit_western_inclusivity.json'.\n"
     ]
    }
   ],
   "source": [
    "import praw\n",
    "import json\n",
    "import csv \n",
    "from datetime import datetime, timezone\n",
    "\n",
    "# Initialize PRAW  Reddit application credentials\n",
    "client = praw.Reddit(\n",
    "    client_id='ZwTyGre7TbxHMGIGVVQYVQ',\n",
    "    client_secret='NbEf85G8ckGdtvShah0-gd0iq8F9uw',\n",
    "    password='Ym9Sf?2gsUK!AsW',\n",
    "    username='Sufficient-Guitar587',\n",
    "    user_agent='client for SNAM2024'\n",
    ")\n",
    "\n",
    "def search_posts(queries, limit=None):\n",
    "    \"\"\"\n",
    "    Searches for posts across all of Reddit that match a list of queries.\n",
    "\n",
    "    Args:\n",
    "        queries (list): The search queries to filter posts.\n",
    "        limit (int): The maximum number of posts to fetch.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of dictionaries where each dictionary contains post data and comments.\n",
    "    \"\"\"\n",
    "    # Initialize list to hold posts\n",
    "    posts = []\n",
    "\n",
    "    try:\n",
    "        # Iterate over each query\n",
    "        for query in queries:\n",
    "            for submission in client.subreddit('all').search(query, sort='relevance', limit=limit):\n",
    "                posts.append(fetch_post_data(submission))\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "    return posts\n",
    "\n",
    "def fetch_post_data(submission):\n",
    "    \"\"\"\n",
    "    Fetches data for a single submission and its comments.\n",
    "\n",
    "    Args:\n",
    "        submission (praw.models.Submission): The submission object to fetch data from.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the post data and comments.\n",
    "    \"\"\"\n",
    "    post_data = {\n",
    "        'title': submission.title,\n",
    "        'score': submission.score,\n",
    "        'created_date': datetime.fromtimestamp(submission.created_utc, tz=timezone.utc).strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        'url': submission.url,\n",
    "        'subreddit': submission.subreddit.display_name,\n",
    "        'comments': []\n",
    "    }\n",
    "\n",
    "    # Fetch all comments from the post\n",
    "    submission.comments.replace_more(limit=0)\n",
    "    for comment in submission.comments.list():\n",
    "        post_data['comments'].append({\n",
    "            'body': comment.body,\n",
    "            'author': comment.author.name if comment.author else '[deleted]',\n",
    "            'created_date': datetime.fromtimestamp(comment.created_utc, tz=timezone.utc).strftime('%Y-%m-%d %H:%M:%S')\n",
    "        })\n",
    "\n",
    "    return post_data\n",
    "\n",
    "def save_to_csv(posts, filename='reddit_data.csv'):\n",
    "    \"\"\"\n",
    "    Saves posts and their comments to a CSV file.\n",
    "\n",
    "    Args:\n",
    "        posts (list): List of posts with comments to save.\n",
    "        filename (str): The name of the CSV file to save the data.\n",
    "    \"\"\"\n",
    "    with open(filename, 'w', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.writer(f)\n",
    "        # Write header in CSV\n",
    "        writer.writerow(['post_title', 'post_score', 'post_created_date', 'post_url', 'post_subreddit', 'comment_body', 'comment_author', 'comment_created_date'])\n",
    "\n",
    "        # Write data\n",
    "        for post in posts:\n",
    "            for comment in post['comments']:\n",
    "                writer.writerow([\n",
    "                    post['title'],\n",
    "                    post['score'],\n",
    "                    post['created_date'],\n",
    "                    post['url'],\n",
    "                    post['subreddit'],\n",
    "                    comment['body'],\n",
    "                    comment['author'],\n",
    "                    comment['created_date']\n",
    "                ])\n",
    "\n",
    "def save_to_json(posts, filename='reddit_data.json'):\n",
    "    \"\"\"\n",
    "    Saves posts and their comments to a JSON file.\n",
    "\n",
    "    Args:\n",
    "        posts (list): List of posts with comments to save.\n",
    "        filename (str): The name of the JSON file to save the data.\n",
    "    \"\"\"\n",
    "    with open(filename, 'w', encoding='utf-8') as json_file:\n",
    "        json.dump(posts, json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "# Example usage\n",
    "queries = [\n",
    "    \"Tirtir inclusivity\",\n",
    "    \"Shiseido inclusivity\",\n",
    "    \"Korean cushion foundation diversity\",\n",
    "    # Add more queries as needed\n",
    "]\n",
    "\n",
    "posts = search_posts(queries, limit=100)\n",
    "\n",
    "# Save the fetched posts to a CSV file\n",
    "save_to_csv(posts, filename='reddit_asian.csv')\n",
    "\n",
    "# Save the fetched posts to a JSON file\n",
    "save_to_json(posts, filename='reddit_asian.json')\n",
    "\n",
    "print(\"Data saved to 'reddit_western_inclusivity.csv' and 'reddit_western_inclusivity.json'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2dfb612-08a6-4d66-850b-82474d71ca02",
   "metadata": {},
   "outputs": [],
   "source": [
    "Western beauty brands diversity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca6e167e-e458-4f6c-b269-74d89afdf113",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to 'reddit_korean_racial_stereotype_extra.csv' and 'reddit_korean_racial_stereotype_extra.json'.\n"
     ]
    }
   ],
   "source": [
    "import praw\n",
    "import csv\n",
    "import json\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "# Initialize PRAW  Reddit application credentials\n",
    "client = praw.Reddit(\n",
    "    client_id='ZwTyGre7TbxHMGIGVVQYVQ',\n",
    "    client_secret='NbEf85G8ckGdtvShah0-gd0iq8F9uw',\n",
    "    password='Ym9Sf?2gsUK!AsW',\n",
    "    username='Sufficient-Guitar587',\n",
    "    user_agent='client for SNAM2024'\n",
    ")\n",
    "\n",
    "def search_posts(query, limit=None):\n",
    "    \"\"\"\n",
    "    Searches for posts across specified subreddits that match a given query.\n",
    "\n",
    "    Args:\n",
    "        query (str): The search query to filter posts.\n",
    "        limit (int): The maximum number of posts to fetch.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of dictionaries where each dictionary contains post data and comments.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize list to hold posts\n",
    "    posts = []\n",
    "\n",
    "    try:\n",
    "        # Search in specified subreddits\n",
    "        # specified_subreddits = [\"AsianBeauty\", \"Skincare\", \"Beauty\"]\n",
    "        # for subreddit_name in specified_subreddits:\n",
    "        #     subreddit = client.subreddit(subreddit_name)\n",
    "        #     for submission in subreddit.search(query, sort='relevance', limit=limit):\n",
    "        #         posts.append(fetch_post_data(submission))\n",
    "          for submission in client.subreddit(\"all\").search(query, sort='relevance', limit=limit):\n",
    "            posts.append(fetch_post_data(submission))\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "    return posts\n",
    "\n",
    "def fetch_post_data(submission):\n",
    "    \"\"\"\n",
    "    Fetches data for a single submission and its comments.\n",
    "\n",
    "    Args:\n",
    "        submission (praw.models.Submission): The submission object to fetch data from.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the post data and comments.\n",
    "    \"\"\"\n",
    "    post_data = {\n",
    "        'title': submission.title,\n",
    "        'score': submission.score,\n",
    "        'created_date': datetime.fromtimestamp(submission.created_utc, tz=timezone.utc).strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        'url': submission.url,\n",
    "        'subreddit': submission.subreddit.display_name,  # Extract subreddit name\n",
    "        'comments': []\n",
    "    }\n",
    "\n",
    "    # Fetch all comments from the post\n",
    "    submission.comments.replace_more(limit=0)\n",
    "    for comment in submission.comments.list():\n",
    "        post_data['comments'].append({\n",
    "            'body': comment.body,\n",
    "            'author': comment.author.name if comment.author else '[deleted]',  # Extract comment author\n",
    "            'created_date': datetime.fromtimestamp(comment.created_utc, tz=timezone.utc).strftime('%Y-%m-%d %H:%M:%S')\n",
    "        })\n",
    "\n",
    "    return post_data\n",
    "\n",
    "def save_to_csv(posts, filename='reddit_data.csv'):\n",
    "    \"\"\"\n",
    "    Saves posts and their comments to a CSV file.\n",
    "\n",
    "    Args:\n",
    "        posts (list): List of posts with comments to save.\n",
    "        filename (str): The name of the CSV file to save the data.\n",
    "    \"\"\"\n",
    "    with open(filename, 'w', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.writer(f)\n",
    "        # Write header in CSV\n",
    "        writer.writerow(['post_title', 'post_score', 'post_created_date', 'post_url', 'post_subreddit', 'comment_body', 'comment_author', 'comment_created_date'])\n",
    "\n",
    "        # Write data\n",
    "        for post in posts:\n",
    "            for comment in post['comments']:\n",
    "                writer.writerow([\n",
    "                    post['title'],\n",
    "                    post['score'],\n",
    "                    post['created_date'],\n",
    "                    post['url'],\n",
    "                    post['subreddit'],  # Write subreddit information\n",
    "                    comment['body'],\n",
    "                    comment['author'],  # Write comment author\n",
    "                    comment['created_date']\n",
    "                ])\n",
    "\n",
    "def save_to_json(posts, filename='reddit_data.json'):\n",
    "    \"\"\"\n",
    "    Saves posts and their comments to a JSON file.\n",
    "\n",
    "    Args:\n",
    "        posts (list): List of posts with comments to save.\n",
    "        filename (str): The name of the JSON file to save the data.\n",
    "    \"\"\"\n",
    "    with open(filename, 'w', encoding='utf-8') as json_file:\n",
    "        json.dump(posts, json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "# Example usage\n",
    "query = \"fenty beauty inclusivity\"\n",
    "\n",
    "posts = search_posts(query, limit=100)\n",
    "\n",
    "# Save the fetched posts to a CSV file\n",
    "save_to_csv(posts, filename='reddit_western_beauty_inclusivity.csv')\n",
    "\n",
    "# Save the fetched posts to a JSON file\n",
    "save_to_json(posts, filename='reddit_western_beauty_inclusivity.json')\n",
    "\n",
    "print(f\"Data saved to 'reddit_korean_racial_stereotype_extra.csv' and 'reddit_korean_racial_stereotype_extra.json'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1cff5a32-1331-495a-8d60-45ba4c06415f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\joell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\joell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned data saved to 'Q2/reddit_western_beauty_inclusivity_cleaned.json'.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Ensure NLTK resources are available\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Initialize Lemmatizer and Stopwords\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    # Remove non-alphanumeric characters\n",
    "    text = re.sub(r'\\W', ' ', text)\n",
    "    # Tokenize, lemmatize, and remove stop words\n",
    "    tokens = text.split()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words]\n",
    "    # Join the tokens back into a string\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "def preprocess_file(input_filename, output_filename):\n",
    "    \"\"\"\n",
    "    Reads a JSON file, preprocesses the relevant fields, and saves the cleaned data to a new JSON file.\n",
    "\n",
    "    Args:\n",
    "        input_filename (str): The name of the input JSON file.\n",
    "        output_filename (str): The name of the output JSON file to save the cleaned data.\n",
    "    \"\"\"\n",
    "    # Load data from the input JSON file\n",
    "    with open(input_filename, 'r', encoding='utf-8') as json_file:\n",
    "        data = json.load(json_file)\n",
    "\n",
    "    # Preprocess the posts and comments\n",
    "    for post in data:\n",
    "        post['title'] = preprocess_text(post['title'])  # Preprocess title\n",
    "        for comment in post['comments']:\n",
    "            comment['body'] = preprocess_text(comment['body'])  # Preprocess comment body\n",
    "\n",
    "    # Save the cleaned data to a new JSON file\n",
    "    with open(output_filename, 'w', encoding='utf-8') as json_file:\n",
    "        json.dump(data, json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "# Example usage\n",
    "input_file = 'Q2/reddit_western_beauty_inclusivity.json'  # Put your input file name here\n",
    "output_file = 'Q2/reddit_western_beauty_inclusivity_cleaned.json'  # Specify the output file name here\n",
    "\n",
    "preprocess_file(input_file, output_file)\n",
    "\n",
    "print(f\"Cleaned data saved to '{output_file}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda4dffe-1f18-42a5-a781-7f758a000e99",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
